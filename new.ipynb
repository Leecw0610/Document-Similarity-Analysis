{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "887dd2983f7208988a626399d0ca0500fea739fc46d2d66648f883e679c7300b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Word2Vec 구현하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize & Vectorize\n",
    "from ckonlpy.tag import Twitter\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (참고) 데이터와 분석방향에 대한 간단한 설명\n",
    "\n",
    "# 데이터: 약 2000여개의 책 데이터\n",
    "# 분석방향: 책의 내용('context')을 분석해 책 A와 책 B가 얼마나 유사한지 \"코사인 유사도\"로 나타내고자 함\n",
    "\n",
    "# ex. A라는 책과 B라는 책은 얼마나 유사할까?\n",
    "# 책의 내용이 담긴 'context' 컬럼을 분석해 어떤 책끼리 유사한지 확인해보는 것이 목표이다\n",
    "# [1. 전처리 2. 형태소 분석 3. word2vec모델에서 훈련 4. 코사인 유사도 분석] 순으로 진행한다"
   ]
  },
  {
   "source": [
    "### 1. 데이터 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 호출\n",
    "df = pd.read_csv('book.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열만 뽑기\n",
    "df_new = df_merge[['title', 'author', 'company', 'year', 'genre', 'context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 처리\n",
    "df_new['context'] = df_new['context'].str.replace(pat=r'[-=,/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》■□●○◆①②③④【】▶]', repl=r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄바꿈 문자 처리\n",
    "df_new['context'] = df_new['context'].str.replace(\"\\n\", \"\")"
   ]
  },
  {
   "source": [
    "### 2. 형태소 분석하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 twitter 사용을 위한 경로추가\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import ckonlpy\n",
    "print('ckonlpy version = {}'.format(ckonlpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 사전 추가 : twitter가 인식하지 못하는 단어들을 새롭게 정의하기\n",
    "twitter = Twitter(use_twitter_dictionary=False)\n",
    "twitter.add_dictionary(['머신러닝','인공지능', '자율주행'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "# 형태소 분석에서 제외하고 싶은 단어를 정의\n",
    "stopwords = ['하지만', '그래서', '특히']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 형태소 단위로 토큰화하는 함수 생성\n",
    "def tokenizing(concat_str):\n",
    "    \n",
    "    temp_X = twitter.pos(concat_str, stem=True, norm=True)          # 단어 토큰화: 품사도 함께 표시되도록\n",
    "\n",
    "    temp_X = [word[0] for word in temp_X if word[1] in [\"Noun\"]]    # 명사만 저장\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]     # 불용어 제거\n",
    "    temp_X = [word for word in temp_X if len(word) > 1]             # 한글자짜리 형태소는 제거\n",
    "    \n",
    "    return temp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'context' 컬럼에 토큰화 함수 적용 후, 결과를 'context_tokenized'이라는 새로운 컬럼에 넣는다.\n",
    "df_new['context_tokenized'] = df_new['context'].apply(lambda x: tokenizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 된 데이터를 tok_result 리스트에 넣는다.\n",
    "tok_result = []\n",
    "append = tok_result.append\n",
    "\n",
    "for i in range(len(df_new['context_tokenized'])):\n",
    "    append(df_new['ALLcontext_tokenizedtokenize'].values[i])"
   ]
  },
  {
   "source": [
    "### 3. Word2Vec 모델 훈련시키기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 호출\n",
    "# https://ratsgo.github.io/embedding/downloaddata.html 에서 다운로드한 word2vec 모델을 사용\n",
    "# (4. 단어 임베딩 > '이곳' 클릭해 zip 다운로드 > word2vec 폴더 속 'word2vec'을 사용함)\n",
    "model = gensim.models.Word2Vec.load('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 모델에 우리가 토큰화한 단어를 훈련시킨다.\n",
    "model = Word2Vec(tok_result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개수 확인\n",
    "total_examples = model.corpus_count\n",
    "print(total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 추가훈련\n",
    "# 위의 모델에는 없는 단어를 훈련시켜 성능을 향상시키기 위해 추가훈련을 진행한다.\n",
    "# get_wiki_corpus.ipynb 에서 만든 모델을 불러온다 (약 53만건의 데이터를 포함하고 있는 모델)\n",
    "new_model = gensim.models.Word2Vec.load('wiki_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델에서 단어사전을 생성해 기존 모델에 붙이기\n",
    "model.build_vocab([list(new_model.wv.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 합치기\n",
    "model.intersect_word2vec_format(\"ko.bin\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝\n",
    "model.train(result, total_examples=total_examples, epochs=10)"
   ]
  },
  {
   "source": [
    "### 4. 단어 임베딩"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model로부터 단어벡터 구하기\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(word_vectors.vocab.keys())              # 단어\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs] # 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리 형태로 만들기: {'단어' : 벡터값}\n",
    "vocab_vector_dict = dict(zip(vocabs, word_vectors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터의 평균 구하기\n",
    "def get_avg(context_tokenized):\n",
    "    base_vector = np.zeros(shape = (100,))\n",
    "    cnt = 0\n",
    "    for token in context_tokenized:\n",
    "        try:\n",
    "            base_vector += vocab_vector_dict[token]\n",
    "            cnt += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return base_vector/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용 : 'context_vector' 컬럼에 벡터값을 저장한다\n",
    "df_new['context_vector'] = df_new['context_tokenized'].apply(lambda x: get_avg(x))"
   ]
  },
  {
   "source": [
    "### 5. 코사인 유사도 계산"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 구하는 함수 생성\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B) / (norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종적으로 생성하고자 하는 데이터프레임\n",
    "final_df = pd.DataFrame(columns=['---1', '---2', '---3', '---4', '유사도'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(len(df_new))):\n",
    "    # 비교하고자 하는 데이터의 벡터값\n",
    "    all_query_vector = df_new['context_vector'].values[n]\n",
    "\n",
    "    # 같은 genre에 속하는 데이터끼리는 유사도를 계산하지 않는다.\n",
    "    all_cos_list = []\n",
    "    append = all_cos_list.append\n",
    "    for context_vector in (df_new['context_vector'][df_new['genre'] != df_new.iloc[n]['genre']]).values:\n",
    "        append(cos_sim(all_query_vector, context_vector))\n",
    "\n",
    "    # 결과 DataFrame 생성\n",
    "    result_df = pd.DataFrame(columns=['title', 'author', 'company', 'year', 'genre', '유사도'])\n",
    "\n",
    "    # 결과로 도출된 데이터\n",
    "    result_df['title'] = (df_new['title'][df_new['genre'] != df_new.iloc[n]['genre']]).values\n",
    "    result_df['author'] = (df_new['author'][df_new['genre'] != df_new.iloc[n]['genre']]).values\n",
    "    result_df['company'] = (df_new['company'][df_new['genre'] != df_new.iloc[n]['genre']]).values\n",
    "    result_df['year'] = (df_new['year'][df_new['genre'] != df_new.iloc[n]['genre']]).values\n",
    "    result_df['유사도'] = all_cos_list\n",
    "\n",
    "    # 유사도 값 채우기\n",
    "    result_df = result_df[np.isfinite(result_df['유사도'])]\n",
    "    result_df = result_df.sort_values(by=['유사도'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # 비교하고자 하는 데이터\n",
    "    result_df['제목'] = df_new['제목'].values[n]\n",
    "    result_df['작가'] = df_new['작가'].values[n]\n",
    "    result_df['출판사'] = df_new['출판사'].values[n]\n",
    "    result_df['출판년도'] = df_new['출판년도'].values[n]\n",
    "\n",
    "    # 유사도 0.90 이상의 데이터만 데이터프레임으로 저장한다.\n",
    "    top_df = result_df.loc[result_df['유사도'] >= 0.90]\n",
    "\n",
    "    # 생성된 데이터프레임을 합친다.\n",
    "    final_df = pd.concat([final_df, top_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 높은 순으로 정렬\n",
    "final_df.sort_values(['유사도'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 인덱스 번호로 되어있는 것을 리셋\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 데이터의 크기 확인\n",
    "len(final_df)"
   ]
  }
 ]
}